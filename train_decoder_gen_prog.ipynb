{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TrainerModule:\n",
    "\n",
    "    def __init__(self, model_name, exmp_batch, max_iters, lr=1e-3, warmup=100, seed=42, **model_kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            model_name - Name of the model. Used for saving and checkpointing\n",
    "            exmp_batch - Example batch to the model for initialization\n",
    "            max_iters - Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n",
    "            lr - Learning rate in the optimizer\n",
    "            warmup - Number of warmup steps. Usually between 50 and 500\n",
    "            seed - Seed to use for model init\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.max_iters = max_iters\n",
    "        self.lr = lr\n",
    "        self.warmup = warmup\n",
    "        self.seed = seed\n",
    "        # Create empty model. Note: no parameters yet\n",
    "        self.model = EncoderDecoder(**model_kwargs)\n",
    "        # Prepare logging\n",
    "        self.log_dir = os.path.join(CHECKPOINT_PATH, self.model_name)\n",
    "        self.logger = SummaryWriter(log_dir=self.log_dir)\n",
    "        # Create jitted training and eval functions\n",
    "        self.create_functions()\n",
    "        # Initialize model\n",
    "        self.init_model(exmp_batch)\n",
    "\n",
    "    def batch_to_input(self, batch):\n",
    "        # Map batch to input data to the model\n",
    "        inp_data, _ = batch\n",
    "        return inp_data\n",
    "    \n",
    "\n",
    "    \n",
    "    def get_accuracy_function(self):\n",
    "        def accuracy(logits, labels):\n",
    "          def logit_classes_jnp(logits):\n",
    "            classes = [] #jnp.zeros((logits.shape[0], 5))\n",
    "            logits = jnp.array(logits)\n",
    "            \n",
    "            ptr = 0\n",
    "            for i, seg_size in enumerate(dataset.segment_sizes):\n",
    "                #classes[:, i] = logits[:, ptr:ptr + seg_size].argmax(axis=1)\n",
    "                classes.append(logits[:, :, ptr:ptr + seg_size].argmax(axis=2))\n",
    "                ptr += seg_size\n",
    "            classes = jnp.stack(classes, axis=2)\n",
    "            return classes\n",
    "          classes = logit_classes_jnp(logits)\n",
    "          \n",
    "          time_steps = logits.shape[1]\n",
    "          acc = (classes == labels[:, :time_steps, :]).mean()\n",
    "          return acc\n",
    "        return accuracy\n",
    "\n",
    "    def get_loss_function(self):\n",
    "        # Function for calculating loss and accuracy for a batch\n",
    "        def calculate_loss(params, rng, batch, train):\n",
    "            # Input data has shape (batch_size, time_steps, features)\n",
    "            # Labels has shape (batch_size, time_steps, 5)\n",
    "            inp_data, labels = batch\n",
    "            time_steps = inp_data.shape[1]\n",
    "            rng, dropout_apply_rng = random.split(rng)\n",
    "            logits = self.model.apply({'params': params}, inp_data, train=train, rngs={'dropout': dropout_apply_rng})\n",
    "\n",
    "            ptr = 0\n",
    "            loss = 0\n",
    "            for i, seg_size in enumerate(dataset.segment_sizes):\n",
    "                loss += optax.softmax_cross_entropy_with_integer_labels(logits[:, :, ptr:ptr + seg_size], labels[:, :time_steps, i])\n",
    "                ptr += seg_size\n",
    "\n",
    "            #loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
    "            loss = loss.mean()\n",
    "            acc = self.accuracy_fn(logits, labels)\n",
    "            return loss, (acc, rng)\n",
    "        return calculate_loss\n",
    "\n",
    "    def create_functions(self):\n",
    "        # Create jitted train and eval functions\n",
    "        calculate_loss = self.get_loss_function()\n",
    "\n",
    "        # Training function\n",
    "        def train_step(state, rng, batch):\n",
    "            loss_fn = lambda params: calculate_loss(params, rng, batch, train=True)\n",
    "            ret, grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "            loss, acc, rng = ret[0], *ret[1]\n",
    "            state = state.apply_gradients(grads=grads)\n",
    "            return state, rng, loss, acc\n",
    "        self.train_step = jax.jit(train_step)\n",
    "\n",
    "        # Evaluation function\n",
    "        def eval_step(state, rng, batch):\n",
    "            _, (acc, rng) = calculate_loss(state.params, rng, batch, train=False)\n",
    "            return acc, rng\n",
    "        self.eval_step = jax.jit(eval_step)\n",
    "\n",
    "        self.accuracy_fn = self.get_accuracy_function()\n",
    "\n",
    "    def init_model(self, exmp_batch):\n",
    "        # Initialize model\n",
    "        self.rng = jax.random.PRNGKey(self.seed)\n",
    "        self.rng, init_rng, dropout_init_rng = jax.random.split(self.rng, 3)\n",
    "        exmp_input = self.batch_to_input(exmp_batch)\n",
    "        params = self.model.init({'params': init_rng, 'dropout': dropout_init_rng}, exmp_input, train=True)['params']\n",
    "        # Initialize learning rate schedule and optimizer\n",
    "        lr_schedule = optax.warmup_cosine_decay_schedule(\n",
    "            init_value=0.0,\n",
    "            peak_value=self.lr,\n",
    "            warmup_steps=self.warmup,\n",
    "            decay_steps=self.max_iters,\n",
    "            end_value=0.0\n",
    "        )\n",
    "        optimizer = optax.chain(\n",
    "            optax.clip_by_global_norm(1.0),  # Clip gradients at norm 1\n",
    "            optax.adam(lr_schedule)\n",
    "        )\n",
    "        # Initialize training state\n",
    "        self.state = train_state.TrainState.create(apply_fn=self.model.apply, params=params, tx=optimizer)\n",
    "\n",
    "    def train_model(self, train_loader, val_loader, num_epochs=500):\n",
    "        # Train model for defined number of epochs\n",
    "        best_acc = 0.0\n",
    "        for epoch_idx in range(1, num_epochs+1):\n",
    "            self.train_epoch(train_loader, epoch=epoch_idx)\n",
    "            if epoch_idx % 5 == 0:\n",
    "                eval_acc = self.eval_model(val_loader)\n",
    "                self.logger.add_scalar('val/accuracy', eval_acc, global_step=epoch_idx)\n",
    "                if eval_acc >= best_acc:\n",
    "                    best_acc = eval_acc\n",
    "                    self.save_model(step=epoch_idx)\n",
    "                self.logger.flush()\n",
    "\n",
    "    def train_epoch(self, train_loader, epoch):\n",
    "        # Train model for one epoch, and log avg loss and accuracy\n",
    "        with tqdm(total=len(train_loader), unit='batch') as tepoch:\n",
    "          tepoch.set_description(f\"Epoch {epoch}\")\n",
    "          \n",
    "          accs, losses = [], []\n",
    "          for idx, batch in enumerate(train_loader):\n",
    "              self.state, self.rng, loss, accuracy = self.train_step(self.state, self.rng, batch)\n",
    "              losses.append(loss)\n",
    "              accs.append(accuracy)\n",
    "\n",
    "              tepoch.set_postfix({'Batch': idx, 'Train Loss': loss.item(), 'Acc': accuracy.item()})\n",
    "              tepoch.update(1)\n",
    "\n",
    "          avg_loss = np.stack(jax.device_get(losses)).mean()\n",
    "          avg_acc = np.stack(jax.device_get(accs)).mean()\n",
    "          self.logger.add_scalar('train/loss', avg_loss, global_step=epoch)\n",
    "          self.logger.add_scalar('train/accuracy', avg_acc, global_step=epoch)\n",
    "\n",
    "    def eval_model(self, data_loader):\n",
    "        # Test model on all data points of a data loader and return avg accuracy\n",
    "        correct_class, count = 0, 0\n",
    "        for batch in data_loader:\n",
    "            acc, self.rng = self.eval_step(self.state, self.rng, batch)\n",
    "            correct_class += acc * batch[0].shape[0]\n",
    "            count += batch[0].shape[0]\n",
    "        eval_acc = (correct_class / count).item()\n",
    "        return eval_acc\n",
    "\n",
    "    def save_model(self, step=0):\n",
    "        # Save current model at certain training iteration\n",
    "        checkpoints.save_checkpoint(ckpt_dir=self.log_dir, target=self.state.params, step=step)\n",
    "\n",
    "    def load_model(self, pretrained=False):\n",
    "        # Load model. We use different checkpoint for the pretrained model\n",
    "        if not pretrained:\n",
    "            params = checkpoints.restore_checkpoint(ckpt_dir=self.log_dir, target=self.state.params)\n",
    "        else:\n",
    "            params = checkpoints.restore_checkpoint(ckpt_dir=os.path.join(CHECKPOINT_PATH, f'{self.model_name}.ckpt'), target=self.state.params)\n",
    "        self.state = train_state.TrainState.create(apply_fn=self.model.apply, params=params, tx=self.state.tx)\n",
    "\n",
    "    def checkpoint_exists(self):\n",
    "        # Check whether a pretrained model exist for this Transformer\n",
    "        return os.path.isfile(os.path.join(CHECKPOINT_PATH, f'{self.model_name}.ckpt'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from models import EncoderDecoder, TransformerEncoder, EncoderBlock\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "from utils.jax_helpers import JaxSeeder\n",
    "import os\n",
    "CHECKPOINT_PATH = \".logs/\"\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import jax\n",
    "import optax\n",
    "from flax.training import train_state, checkpoints\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from functools import partial\n",
    "\n",
    "torch.cuda.is_available = lambda : False\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from program_dataloader import TorchProgramDataset\n",
    "\n",
    "dataset = TorchProgramDataset()\n",
    "collate_fn = partial(TorchProgramDataset.collate_fn, dataset.prog_len)\n",
    "train_dataloader = DataLoader(dataset, batch_size=64, collate_fn=collate_fn, num_workers=8, prefetch_factor=2)#, pin_memory=True)\n",
    "\n",
    "it = iter(train_dataloader)\n",
    "x,y = next(it)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:Tensorflow library not found, tensorflow.io.gfile operations will use native shim calls. GCS paths (i.e. 'gs://...') cannot be accessed.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "print(dataset.decode_pred(y, 0))\n",
    "print(dataset.decode_pred(x, 0))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PROGRAM_START\n",
      "Map LAM_MUL v1 NA v2\n",
      "Map LAM_MUL indices NA v3\n",
      "Aggregate v4 v3 NA v5\n",
      "SequenceMap LAM_OR v6 indices v7\n",
      "Map LAM_ADD v7 NA v8\n",
      "Map LAM_SUB v8 NA v9\n",
      "Map LAM_SUB v9 NA v10\n",
      "SequenceMap LAM_OR v10 v5 v1\n",
      "Map LAM_ADD indices NA v6\n",
      "Map LAM_LT v6 NA v11\n",
      "Select v11 v11 PRED_NEQ v4\n",
      "Aggregate v4 v1 NA v12\n",
      "Map LAM_SUB v12 NA v13\n",
      "SequenceMap LAM_SUB v13 v2 v14\n",
      "PROGRAM_END\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "\n",
      "PROGRAM_START\n",
      "Map LAM_MUL v1 NA v2\n",
      "Map LAM_MUL indices NA v3\n",
      "Aggregate v4 v3 NA v5\n",
      "SequenceMap LAM_OR v6 indices v7\n",
      "Map LAM_ADD v7 NA v8\n",
      "Map LAM_SUB v8 NA v9\n",
      "Map LAM_SUB v9 NA v10\n",
      "SequenceMap LAM_OR v10 v5 v1\n",
      "Map LAM_ADD indices NA v6\n",
      "Map LAM_LT v6 NA v11\n",
      "Select v11 v11 PRED_NEQ v4\n",
      "Aggregate v4 v1 NA v12\n",
      "Map LAM_SUB v12 NA v13\n",
      "SequenceMap LAM_SUB v13 v2 v14\n",
      "PROGRAM_END\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# for i in range(0,1000):\n",
    "#     x,y = next(it)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "max_epochs = 50\n",
    "num_train_iters = len(train_dataloader) * max_epochs\n",
    "\n",
    "trainer = TrainerModule('Program-Encoder-Decoder', next(it), num_train_iters, num_classes=sum(dataset.segment_sizes))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "trainer.load_model()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "it = iter(train_dataloader)\n",
    "x,y = next(it)\n",
    "print(dataset.decode_pred(y, 0))\n",
    "print(dataset.decode_pred(x, 0))\n",
    "\n",
    "\n",
    "rng, dropout_apply_rng = random.split(trainer.rng)\n",
    "logits = trainer.model.apply({'params': trainer.state.params}, x, train=False, rngs={'dropout': dropout_apply_rng})\n",
    "\n",
    "print(dataset.decode_pred(logits, 0))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PROGRAM_START\n",
      "SequenceMap LAM_AND v1 v1 v2\n",
      "Select v3 v1 PRED_LT v4\n",
      "SelectorWidth v4 NA NA v5\n",
      "SequenceMap LAM_ADD v5 v2 v6\n",
      "SequenceMap LAM_ADD v7 v7 v3\n",
      "Aggregate v8 v3 NA v9\n",
      "SequenceMap LAM_MUL v9 v1 v10\n",
      "Select v10 v11 PRED_EQ v12\n",
      "Aggregate v12 v6 NA v13\n",
      "SequenceMap LAM_SUB indices indices v14\n",
      "SequenceMap LAM_OR v14 v14 v15\n",
      "SequenceMap LAM_MUL v15 v15 v11\n",
      "Map LAM_ADD v11 NA v7\n",
      "Select v7 v7 PRED_LEQ v8\n",
      "SelectorWidth v8 NA NA v1\n",
      "Map LAM_MUL v1 NA v16\n",
      "PROGRAM_END\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "\n",
      "PROGRAM_START\n",
      "SequenceMap LAM_AND v1 v1 v2\n",
      "Select v3 v1 PRED_LT v4\n",
      "SelectorWidth v4 NA NA v5\n",
      "SequenceMap LAM_ADD v5 v2 v6\n",
      "SequenceMap LAM_ADD v7 v7 v3\n",
      "Aggregate v8 v3 NA v9\n",
      "SequenceMap LAM_MUL v9 v1 v10\n",
      "Select v10 v11 PRED_EQ v12\n",
      "Aggregate v12 v6 NA v13\n",
      "SequenceMap LAM_SUB indices indices v14\n",
      "SequenceMap LAM_OR v14 v14 v15\n",
      "SequenceMap LAM_MUL v15 v15 v11\n",
      "Map LAM_ADD v11 NA v7\n",
      "Select v7 v7 PRED_LEQ v8\n",
      "SelectorWidth v8 NA NA v1\n",
      "Map LAM_MUL v1 NA v16\n",
      "PROGRAM_END\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "\n",
      "PROGRAM_START\n",
      "SequenceMap LAM_AND v1 v1 v2\n",
      "Select v3 v1 PRED_LT v4\n",
      "SelectorWidth v4 NA NA v5\n",
      "SequenceMap LAM_ADD v5 v2 v6\n",
      "SequenceMap LAM_ADD v7 v7 v3\n",
      "Aggregate v8 v3 NA v9\n",
      "SequenceMap LAM_MUL v9 v1 v10\n",
      "Select v10 v11 PRED_EQ v12\n",
      "Aggregate v12 v6 NA v13\n",
      "SequenceMap LAM_SUB indices indices v14\n",
      "SequenceMap LAM_OR v14 v14 v15\n",
      "SequenceMap LAM_MUL v15 v15 v11\n",
      "Map LAM_ADD v11 NA v7\n",
      "Select v7 v7 PRED_LEQ v8\n",
      "SelectorWidth v8 NA NA v1\n",
      "Map LAM_MUL v1 NA v16\n",
      "PROGRAM_END\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "<PAD>\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}